#pylint: disable=too-many-locals, too-many-arguments
"""
    Class to execute and hold the off-specular reflectivity calculation.
"""
import logging
import numpy as np
import scipy.stats
from multiprocessing import Pool

from reflectivity_ui.interfaces.configuration import Configuration

H_OVER_M_NEUTRON = 3.956034e-7 # h/m_n [m^2/s]


class OffSpecular(object):
    """
        Compute off-specular reflectivity
    """
    d_wavelength = 0
    Qx = None
    Qz = None
    ki_z = None
    kf_z = None
    S = None
    dS = None

    def __init__(self, cross_section_data):
        """
            :param CrossSectionData cross_section_data: processed data object
        """
        self.data_set = cross_section_data

    def __call__(self, direct_beam=None):
        """
            Extract off-specular scattering from 4D dataset (x,y,ToF,I).
            Uses a window in y to filter the 4D data
            and than sums all I values for each ToF and x channel.
            Qz,Qx,kiz,kfz is calculated using the x and ToF positions
            together with the tth-bank and direct pixel values.

            :param CrossSectionData direct_beam: if given, this data will be used to normalize the output
        """
        #TODO: correct for detector sensitivity
        x_pos = self.data_set.configuration.peak_position
        scale = 1./self.data_set.proton_charge

        # Range in low-res direction
        y_min, y_max = self.data_set.configuration.low_res_roi

        rad_per_pixel = self.data_set.det_size_x / self.data_set.dist_sam_det / self.data_set.xydata.shape[1]

        xtth = self.data_set.direct_pixel - np.arange(self.data_set.data.shape[0])[self.data_set.active_area_x[0]:
                                                                                   self.data_set.active_area_x[1]]
        pix_offset_spec = self.data_set.direct_pixel - x_pos
        delta_dangle = self.data_set.dangle - self.data_set.angle_offset
        tth_spec = delta_dangle * np.pi/180. + pix_offset_spec * rad_per_pixel
        af = delta_dangle * np.pi/180. + xtth * rad_per_pixel - tth_spec/2.
        ai = np.ones_like(af) * tth_spec / 2.

        # Background
        bck = self.data_set.get_background_vs_TOF() * scale

        v_edges = self.data_set.dist_mod_det/self.data_set.tof_edges * 1e6 #m/s
        lambda_edges = H_OVER_M_NEUTRON / v_edges * 1e10 #A

        wl = (lambda_edges[:-1] + lambda_edges[1:]) / 2.
        # The resolution for lambda is digital range with equal probability
        # therefore it is the bin size divided by sqrt(12)
        self.d_wavelength = np.abs(lambda_edges[:-1] - lambda_edges[1:]) / np.sqrt(12)
        k = 2. * np.pi / wl

        # calculate reciprocal space, incident and outgoing perpendicular wave vectors
        self.Qz = k[np.newaxis, :] * (np.sin(af)+np.sin(ai))[:, np.newaxis]
        self.Qx = k[np.newaxis, :] * (np.cos(af)-np.cos(ai))[:, np.newaxis]
        self.ki_z = k[np.newaxis, :] * np.sin(ai)[:, np.newaxis]
        self.kf_z = k[np.newaxis, :] * np.sin(af)[:, np.newaxis]

        # calculate ROI intensities and normalize by number of points
        raw_multi_dim = self.data_set.data[self.data_set.active_area_x[0]:self.data_set.active_area_x[1], y_min:y_max, :]
        raw = raw_multi_dim.sum(axis=1)
        d_raw = np.sqrt(raw)

        # normalize data by width in y and multiply scaling factor
        intensity = raw/float(y_max-y_min) * scale
        d_intensity = d_raw/(y_max-y_min) * scale
        self.S = intensity - bck[np.newaxis, :]
        self.dS = np.sqrt(d_intensity**2+(bck**2)[np.newaxis, :])
        self.S *= self.data_set.configuration.scaling_factor
        self.dS *= self.data_set.configuration.scaling_factor

        if direct_beam is not None:
            if not direct_beam.configuration.tof_bins == self.data_set.configuration.tof_bins:
                logging.error("Trying to normalize with a direct beam data set with different binning")

            norm_y_min, norm_y_max = direct_beam.configuration.low_res_roi
            norm_x_min, norm_x_max = direct_beam.configuration.peak_roi
            norm_raw_multi_dim = direct_beam.data[norm_x_min:norm_x_max,
                                                  norm_y_min:norm_y_max, :]

            norm_raw = norm_raw_multi_dim.sum(axis=0).sum(axis=0)
            norm_d_raw = np.sqrt(norm_raw)
            norm_scale = (float(norm_x_max)-float(norm_x_min)) * (float(norm_y_max)-float(norm_y_min))
            norm_raw /= norm_scale * direct_beam.proton_charge
            norm_d_raw /= norm_scale * direct_beam.proton_charge

            idxs = norm_raw > 0.
            self.dS[:, idxs] = np.sqrt((self.dS[:, idxs]/norm_raw[idxs][np.newaxis, :])**2 +
                                       (self.S[:, idxs]/norm_raw[idxs][np.newaxis, :]**2*norm_d_raw[idxs][np.newaxis, :])**2
                                      )
            self.S[:, idxs] /= norm_raw[idxs][np.newaxis, :]
            self.S[:, np.logical_not(idxs)] = 0.
            self.dS[:, np.logical_not(idxs)] = 0.

def merge(reduction_list, pol_state):
    """
        Merge the off-specular data from a reduction list.
        :param list reduction_list: list of NexusData objects
        :param string pol_state: polarization state to consider

        The scaling factors should have been determined at this point. Just use them
        to merge the different runs in a set.

        TODO: This doesn't deal with the overlap properly. It assumes that the user
        cut the overlapping points by hand.
    """
    _qx = np.empty(0)
    _qz = np.empty(0)
    _ki_z = np.empty(0)
    _kf_z = np.empty(0)
    _s = np.empty(0)
    _ds = np.empty(0)

    for item in reduction_list:
        offspec = item.cross_sections[pol_state].off_spec
        Qx, Qz, ki_z, kf_z, S, dS = (offspec.Qx, offspec.Qz, offspec.ki_z, offspec.kf_z,
                                     offspec.S, offspec.dS)

        n_total = len(S[0])
        p_0 = item.cross_sections[pol_state].configuration.cut_first_n_points
        p_n = n_total-item.cross_sections[pol_state].configuration.cut_last_n_points

        #NOTE: need to unravel the arrays from [TOF][pixel] to [q_points]
        Qx = np.ravel(Qx[:, p_0:p_n])
        Qz = np.ravel(Qz[:, p_0:p_n])
        ki_z = np.ravel(ki_z[:, p_0:p_n])
        kf_z = np.ravel(kf_z[:, p_0:p_n])
        S = np.ravel(S[:, p_0:p_n])
        dS = np.ravel(dS[:, p_0:p_n])

        _qx = np.concatenate((_qx, Qx))
        _qz = np.concatenate((_qz, Qz))
        _ki_z = np.concatenate((_ki_z, ki_z))
        _kf_z = np.concatenate((_kf_z, kf_z))
        _s = np.concatenate((_s, S))
        _ds = np.concatenate((_ds, dS))

    return _qx, _qz, _ki_z, _kf_z, _ki_z-_kf_z, _s, _ds

def closest_bin(q, bin_edges):
    """
        Find closest bin to a q-value
        :param float q: q-value
        :param list bin_edges: list of bin edges
    """
    for i in range(len(bin_edges)):
        if q > bin_edges[i] and q < bin_edges[i+1]:
            return i
    return None

def rebin_extract(reduction_list, pol_state, axes=None, y_list=None, use_weights=True,
                  n_bins_x=350, n_bins_y=350, x_min=-0.015, x_max=0.015, y_min=0, y_max=0.1):
    """
        Rebin off-specular data and extract cut at given Qz values.
        Note: the analysis computers with RHEL7 have Scipy 0.12 installed, which makes
        this code uglier. Refactor once we get a more recent version.
    """
    if not isinstance(y_list, list):
        y_list = []

    Qx, Qz, ki_z, kf_z, delta_k, S, dS = merge(reduction_list, pol_state)

    # Specify how many bins we want in each direction.
    _bins = [n_bins_x, n_bins_y]

    # Specify the axes
    if axes is None:
        axes = reduction_list[0].cross_sections[pol_state].configuration.off_spec_x_axis
    x_label = 'ki_z-kf_z'
    y_label = 'Qz'
    x_values = delta_k
    y_values = Qz
    if axes == Configuration.QX_VS_QZ:
        x_label = 'Qx'
        x_values = Qx
    elif axes == Configuration.KZI_VS_KZF:
        x_label = 'ki_z'
        y_label = 'kf_z'
        x_values = ki_z
        y_values = kf_z

    # Find the indices of S[TOF][main_axis_pixel] where we have non-zero data.
    indices = S > 0
    if use_weights:
        # Compute the weighted average
        # - Weighted sum
        _r = S/dS**2
        statistic, x_edge, y_edge, _ = scipy.stats.binned_statistic_2d(x_values[indices],
                                                                       y_values[indices],
                                                                       _r[indices],
                                                                       statistic='sum',
                                                                       range=[[x_min, x_max], [y_min, y_max]],
                                                                       bins=_bins)
        # - Sum of weights
        _w = 1/dS**2
        w_statistic, _, _, _ = scipy.stats.binned_statistic_2d(x_values[indices], y_values[indices], _w[indices],
                                                               statistic='sum',
                                                               range=[[x_min, x_max], [y_min, y_max]],
                                                               bins=[x_edge, y_edge])

        result = statistic / w_statistic
        result = result.T
        error = np.sqrt(1.0/w_statistic).T
    else:
        # Compute the simple average, with errors
        statistic, x_edge, y_edge, _ = scipy.stats.binned_statistic_2d(x_values[indices],
                                                                       y_values[indices],
                                                                       S[indices],
                                                                       statistic='mean',
                                                                       bins=_bins)
        # Compute the errors
        _w = dS**2
        w_statistic, _, _, _ = scipy.stats.binned_statistic_2d(x_values[indices],
                                                               y_values[indices],
                                                               _w[indices],
                                                               statistic='sum',
                                                               bins=[x_edge, y_edge])

        _c = np.ones(len(x_values))
        counts, _, _, _ = scipy.stats.binned_statistic_2d(x_values[indices],
                                                          y_values[indices],
                                                          _c[indices],
                                                          statistic='sum',
                                                          bins=[x_edge, y_edge])

        result = statistic.T
        error = (np.sqrt(w_statistic) / counts).T

    x_middle = x_edge[:-1] + (x_edge[1] - x_edge[0]) / 2.0
    y_middle = y_edge[:-1] + (y_edge[1] - y_edge[0]) / 2.0

    _q_data = []

    for q in y_list:
        i_q = closest_bin(q, y_edge)
        if error is not None:
            _to_save = np.asarray([x_middle, result[i_q], error[i_q]]).T
        else:
            _to_save = np.asarray([x_middle, result[i_q]]).T
        _q_data.append([_to_save, '%s_%s_%s' % (pol_state, y_label, q)])

    return result, error, x_middle, y_middle, _q_data, [x_label, y_label]


def _smooth_data(x, y, I, sigmas=3., gridx=150, gridy=50,
                sigmax=0.0005, sigmay=0.0005,
                x1=-0.03, x2=0.03, y1=0.0, y2=0.1,
                axis_sigma_scaling=None, xysigma0=0.06, indices=None):
    """
      Smooth a irregular spaced dataset onto a regular grid.
      Takes each intensities with a distance < 3*sigma
      to a given grid point and averages their intensities
      weighted by the gaussian of the distance.

      :param dict settings: Contains options for 'grid', 'sigma' and 'region' of the algorithm
      :param numpy.ndarray x: x-values of the original data
      :param numpy.ndarray y: y-values of the original data
      :param numpy.ndarray I: Intensity values of the original data
      :param float sigmas: Range in units of sigma to search around a grid point
      :param int axis_sigma_scaling: Defines how the sigmas change with the x/y value
      :param float xysigma0: x/y value where the given sigmas are used
      :param list indices: list of indices to run over
    """
    xout=np.linspace(x1, x2, gridx)
    yout=np.linspace(y1, y2, gridy)
    Xout, Yout=np.meshgrid(xout, yout)
    Iout=np.zeros_like(Xout)
    ssigmax, ssigmay=sigmax**2, sigmay**2

    imax=len(Xout)
    for i in range(imax):
        #for j in range(len(Xout[0])):
        for j in range(indices[0], indices[1]):
            xij=Xout[i, j]
            yij=Yout[i, j]
            if axis_sigma_scaling:
                if axis_sigma_scaling==1: xyij=xij
                elif axis_sigma_scaling==2: xyij=yij
                elif axis_sigma_scaling==3: xyij=xij+yij
                if xyij==0:
                    continue
                ssigmaxi=ssigmax/xysigma0*xyij
                ssigmayi=ssigmay/xysigma0*xyij
                rij=(x-xij)**2/ssigmaxi+(y-yij)**2/ssigmayi # normalized distance^2
            else:
                rij=(x-xij)**2/ssigmax+(y-yij)**2/ssigmay # normalized distance^2
            take=np.where(rij<sigmas**2) # take points up to 3 sigma distance
            if len(take[0])==0:
                continue
            Pij=np.exp(-0.5*rij[take])
            Pij/=Pij.sum()
            Iout[i, j]=(Pij*I[take]).sum()
    return Xout, Yout, Iout

def proc(data):
    """
        Serializable function to be called by each thread
    """
    return _smooth_data(x=data['x'], y=data['y'], I=data['I'], sigmas=data['sigmas'],
                        gridx=data['gridx'], gridy=data['gridy'],
                        sigmax=data['sigmax'], sigmay=data['sigmay'],
                        x1=data['x1'], x2=data['x2'], y1=data['y1'], y2=data['y2'],
                        axis_sigma_scaling=data['axis_sigma_scaling'],
                        xysigma0=data['xysigma0'], indices=data['indices'])

def smooth_data(x, y, I, sigmas=3., gridx=150, gridy=50,
                sigmax=0.0005, sigmay=0.0005,
                x1=-0.03, x2=0.03, y1=0.0, y2=0.1,
                axis_sigma_scaling=None, xysigma0=0.06, pool=5):
    """
        Execute legacy smoothing process by spreading it to a pool
        of processes.
    """
    pool = int(pool)
    xout=np.linspace(x1, x2, gridx)
    p = Pool(pool)
    n = len(xout)
    step = int(n/pool)
    indices = [[step*i, step*(i+1)] for i in range(pool)]
    indices[-1][1]=n

    inputs = []
    for i in range(pool):
        _d = dict(x=x, y=y, I=I,
                 sigmas=sigmas, gridx=gridx, gridy=gridy,
                 sigmax=sigmax, sigmay=sigmay,
                 x1=x1, x2=x2, y1=y1, y2=y2,
                 axis_sigma_scaling=axis_sigma_scaling, xysigma0=xysigma0,
                 indices=indices[i])
        inputs.append(_d)

    results = p.map(proc, inputs)
    x_out = results[0][0]
    y_out = results[0][1]
    _data = [r[2] for r in results]
    intensity_out = reduce((lambda x, y: x+y), _data)

    return x_out, y_out, intensity_out
